{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00.Depression data process",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zyntonyson/Erisk2017/blob/master/00_Depression_data_process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPa9epB1-r0j",
        "colab_type": "code",
        "outputId": "df78e401-168c-4082-b8b9-554a918f37ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS4KMT0Myb-F",
        "colab_type": "text"
      },
      "source": [
        "## Verificar que haya concordancia con los archivos xml originales\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re5M4OR7yaJP",
        "colab_type": "code",
        "outputId": "e0a75582-d6e2-4524-fd3a-0bfa2971bff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "source": [
        "import os\n",
        "path_data='/content/drive/My Drive/MCE/Participaciones/05.Estancia Jun19/Practicas NLP/Deteccion de depresion/Depression/reddit2017'\n",
        "#path_data=path_data.replace(\"\\\\\" , \"/\" )+'/'\n",
        "#path_data\n",
        "\n",
        "#Obtener listado del primer chunk y verificar que se encuentre en el resto\n",
        "   \n",
        "individuos_train_p=list(os.listdir(path_data+'/training/positive/chunk_1/'))\n",
        "individuos_train_p= [individuo.replace('1.xml','') for individuo in individuos_train_p if individuo !='desktop.ini']\n",
        "print(len(individuos_train_p))\n",
        "print('\\n Respuestas positvas entrenamiento')   \n",
        "for chunk in range(1,11):\n",
        "    individ_in_chunk_p=os.listdir(path_data+'/training/positive/chunk_'+str(chunk))\n",
        "    individ_in_chunk_p=[individuo.replace(str(chunk)+'.xml','') for individuo in individ_in_chunk_p if individuo !='desktop.ini']\n",
        "    print('chunk:',chunk,'\\t',set(individuos_train_p)==set(individ_in_chunk_p))\n",
        "    #print('\\t',len(individ_in_chunk_p))\n",
        "\n",
        "\n",
        "individuos_train_n=list(os.listdir(path_data+'/training/negative/chunk_1/'))\n",
        "individuos_train_n= [individuo.replace('1.xml','') for individuo in individuos_train_n if individuo !='desktop.ini']\n",
        "print(len(individuos_train_n))\n",
        "print('\\n Respuestas negativas entrenamiento')\n",
        "for chunk in range(1,11):\n",
        "    individ_in_chunk_n=os.listdir(path_data+'/training/negative/chunk_'+str(chunk))\n",
        "    individ_in_chunk_n=[individuo.replace(str(chunk)+'.xml','') for individuo in individ_in_chunk_n if individuo !='desktop.ini']\n",
        "    print('chunk:',chunk,'\\t',set(individuos_train_n)==set(individ_in_chunk_n))\n",
        "    #print(individ_in_chunk_n)\n",
        "    #print(individuos_train_n)\n",
        "\n",
        "\n",
        "individuos_train_t=list(os.listdir(path_data+'/testing/chunk_1/'))\n",
        "individuos_train_t= [individuo.replace('1.xml','') for individuo in individuos_train_t if individuo !='desktop.ini']\n",
        "print(len(individuos_train_t))\n",
        "print('\\n Respuestas test entrenamiento')\n",
        "for chunk in range(1,11):\n",
        "    individ_in_chunk_t=os.listdir(path_data+'/testing/chunk_'+str(chunk))\n",
        "    individ_in_chunk_t=[individuo.replace(str(chunk)+'.xml','') for individuo in individ_in_chunk_t if individuo !='desktop.ini']\n",
        "    print('chunk:',chunk,'\\t',set(individuos_train_t)==set(individ_in_chunk_t))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "83\n",
            "\n",
            " Respuestas positvas entrenamiento\n",
            "chunk: 1 \t True\n",
            "chunk: 2 \t True\n",
            "chunk: 3 \t True\n",
            "chunk: 4 \t True\n",
            "chunk: 5 \t True\n",
            "chunk: 6 \t True\n",
            "chunk: 7 \t True\n",
            "chunk: 8 \t True\n",
            "chunk: 9 \t True\n",
            "chunk: 10 \t True\n",
            "403\n",
            "\n",
            " Respuestas negativas entrenamiento\n",
            "chunk: 1 \t True\n",
            "chunk: 2 \t True\n",
            "chunk: 3 \t True\n",
            "chunk: 4 \t True\n",
            "chunk: 5 \t True\n",
            "chunk: 6 \t True\n",
            "chunk: 7 \t True\n",
            "chunk: 8 \t True\n",
            "chunk: 9 \t True\n",
            "chunk: 10 \t True\n",
            "401\n",
            "\n",
            " Respuestas test entrenamiento\n",
            "chunk: 1 \t True\n",
            "chunk: 2 \t True\n",
            "chunk: 3 \t True\n",
            "chunk: 4 \t True\n",
            "chunk: 5 \t True\n",
            "chunk: 6 \t True\n",
            "chunk: 7 \t True\n",
            "chunk: 8 \t True\n",
            "chunk: 9 \t True\n",
            "chunk: 10 \t True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ChOgACu6zmF",
        "colab_type": "text"
      },
      "source": [
        "## Procesamiento del corpus\n",
        "\n",
        "### Caracter√≠sticas del corpus:\n",
        "\n",
        "1. Post de usuarios de reddit\n",
        "1. Separados en 10 chunks\n",
        "1. Formato XML\n",
        "\n",
        "#### Datos de entrenamiento\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E-NDPHm_HVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import os\n",
        "mypath='/content/drive/My Drive/MCE/Participaciones/05.Estancia Jun19/Practicas NLP/Deteccion de depresion/Actualizacion sep 2019'\n",
        "os.chdir( mypath )\n",
        "from os import listdir\n",
        "from glob import glob # Listar archivos\n",
        "\n",
        "# Scrapping\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "import requests\n",
        "import pandas as pd \n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwmDDa8TAt9o",
        "colab_type": "code",
        "outputId": "6e3dfdd1-229d-41ba-aa7a-bfbcb2811dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "#Preproceso\n",
        "import nltk\n",
        "# Convertir a palabras\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Listado de stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#Steeming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhNJaiRf9Iiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corpus_process_train(nChunks,stemm=True,stopword=True,maxlenword=15):\n",
        "    \n",
        "    #Directorios Raiz\n",
        "    root_p='/content/drive/My Drive/MCE/Participaciones/05.Estancia Jun19/Practicas NLP/Deteccion de depresion/Depression/reddit2017/training/positive/chunk_'\n",
        "    root_n='/content/drive/My Drive/MCE/Participaciones/05.Estancia Jun19/Practicas NLP/Deteccion de depresion/Depression/reddit2017/training/negative/chunk_'\n",
        "    \n",
        "    #Lista de sujetos por tipo de respuesta\n",
        "    subjects_p=[subject.replace('_1.xml','') for subject in listdir(root_p+'1/') if subject !='desktop.ini']\n",
        "    subjects_n=[subject.replace('_1.xml','') for subject in listdir(root_n+'1/') if subject !='desktop.ini']\n",
        "    \n",
        "    #DataFrame contenedor\n",
        "    # Se crea en un principio unicamente con los ID's de los sujetos y su etiqueta\n",
        "    df_train=pd.DataFrame(subjects_p+subjects_n,columns=['Id']) # Id_sujetos\n",
        "    df_train['Depress']=len(subjects_p)*[1]+ len(subjects_n)*[0] #Etiqueta depression\n",
        "    \n",
        "    # Procesar cada chunk para generar dataglobal\n",
        "    \n",
        "    for chunk in range(1,nChunks+1):\n",
        "        subjects_p_path=[root_p+str(chunk)+'/'+ subject+'_'+str(chunk)+'.xml' for subject in subjects_p]\n",
        "        subjects_n_path=[root_n+str(chunk)+'/'+ subject+'_'+str(chunk)+'.xml' for subject in subjects_n]\n",
        "        subjects_train_path= subjects_p_path +subjects_n_path\n",
        "        chunk_posts=[]\n",
        "        for subject in subjects_train_path:\n",
        "            posts_subject=[]\n",
        "            infile=open(subject,'r',encoding='utf8')\n",
        "            post_raw=Soup(infile.read())\n",
        "            posts=post_raw.find_all('text')               #Obtener todos los post\n",
        "            for post in posts:\n",
        "                if post.getText()!=None:\n",
        "                    posts_subject.append(post.getText())\n",
        "                    \n",
        "#########################################Preproceso############################################################\n",
        "            # 1.Minusculas  2. Stemming 3. Solo palabras        \n",
        "            all_post_subject=' '.join(posts_subject)\n",
        "            #all_post_subject=re.sub(r'http\\S+', '', all_post_subject)  #Eliminar direcciones web is.alpha() realiza la tarea\n",
        "            tokens_all_post_subject=word_tokenize(all_post_subject)    #Tokenizar\n",
        "            tokens_subject=[token.lower()  for token in tokens_all_post_subject]\n",
        "            if stemm:\n",
        "              if stopword:\n",
        "                tokens_subject=[ps.stem(token) for token in tokens_subject if token.isalpha() and not token in stop_words and len(token)<maxlenword]\n",
        "              else:\n",
        "                tokens_subject=[ps.stem(token) for token in tokens_subject if token.isalpha() and len(token)<maxlenword]\n",
        "            else:\n",
        "              if stopword:\n",
        "                tokens_subject=[token for token in tokens_subject if token.isalpha() and not token in stop_words and len(token)<maxlenword]\n",
        "              else:\n",
        "                tokens_subject=[token for token in tokens_subject if token.isalpha() and len(token)<maxlenword]\n",
        "            post_clean=' '.join(tokens_subject)  # Unir todos los post\n",
        "            chunk_posts.append(post_clean)\n",
        "        name_chunk='Chunk_'+str(chunk)\n",
        "        df_train[name_chunk]=chunk_posts\n",
        "   \n",
        "    return df_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOYHID8ADYK_",
        "colab_type": "text"
      },
      "source": [
        "#### Datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNDAQEr2DXEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def corpus_process_test(nChunks,stemm=True,stopword=True,maxlenword=15):\n",
        "    #Procesar labels\n",
        "    dir_data_test='/content/drive/My Drive/MCE/Participaciones/05.Estancia Jun19/Practicas NLP/Deteccion de depresion/Depression/reddit2017/test_golden_truth.txt'\n",
        "    y_test = pd.read_csv(dir_data_test, header = None, sep= ' ')\n",
        "    y_test=y_test.drop([0], axis=1)\n",
        "    y_test.columns=['Id','Depress']\n",
        "\n",
        "    #Directorio Raiz\n",
        "    root_p='/content/drive/My Drive/MCE/Participaciones/05.Estancia Jun19/Practicas NLP/Deteccion de depresion/Depression/reddit2017/testing/chunk_'\n",
        "    #Lista de sujetos por tipo de respuesta\n",
        "    subjects_t=[subject.replace('_1.xml','') for subject in listdir(root_p+'1/') if subject !='desktop.ini']\n",
        "    \n",
        "    #DataFrame contenedor\n",
        "    df_train=pd.DataFrame(subjects_t,columns=['Id']) # Id_sujetos\n",
        "    \n",
        "    # Procesar cada chunk para generar dataglobal\n",
        "    \n",
        "    for chunk in range(1,nChunks+1):\n",
        "        subjects_t_path=[root_p+str(chunk)+'/'+ subject+'_'+str(chunk)+'.xml' for subject in subjects_t]\n",
        "        chunk_posts=[]\n",
        "        for subject in subjects_t_path:\n",
        "            posts_subject=[]\n",
        "            infile=open(subject,'r',encoding='utf8')\n",
        "            post_raw=Soup(infile.read())\n",
        "            posts=post_raw.find_all('writing')               #Obtener todos los post\n",
        "            for post in posts:\n",
        "              if(post.find('title').getText()!=None):\n",
        "                posts_subject.append(post.find('title').getText())\n",
        "              if(post.find('text').getText()!=None):\n",
        "                posts_subject.append(post.find('text').getText())                    \n",
        "#########################################Preproceso############################################################\n",
        "            # 1.Minusculas  2. Stemming 3. Solo palabras        \n",
        "            all_post_subject=' '.join(posts_subject)\n",
        "            #all_post_subject=re.sub(r'http\\S+', '', all_post_subject)  #Eliminar direcciones web, .isalpha() realiza la tarea\n",
        "            tokens_all_post_subject=word_tokenize(all_post_subject)    #Tokenizar\n",
        "            tokens_subject=[token.lower()  for token in tokens_all_post_subject]\n",
        "            if stemm:\n",
        "              if stopword:\n",
        "                tokens_subject=[ps.stem(token) for token in tokens_subject if token.isalpha() and not token in stop_words and len(token)<maxlenword]\n",
        "              else:\n",
        "                tokens_subject=[ps.stem(token) for token in tokens_subject if token.isalpha() and len(token)<maxlenword]\n",
        "            else:\n",
        "              if stopword:\n",
        "                tokens_subject=[token for token in tokens_subject if token.isalpha() and not token in stop_words and len(token)<maxlenword]\n",
        "              else:\n",
        "                tokens_subject=[token for token in tokens_subject if token.isalpha() and len(token)<maxlenword]\n",
        "            post_clean=' '.join(tokens_subject)  # Unir todos los post\n",
        "            chunk_posts.append(post_clean)\n",
        "        name_chunk='Chunk_'+str(chunk)\n",
        "        df_train[name_chunk]=chunk_posts\n",
        "    df_test=pd.merge(y_test,df_train,on='Id',how='inner')\n",
        "    return df_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz1CfguurU6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train=corpus_process_train(10,stemm=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKx22GbdJFMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test=corpus_process_test(10,stemm=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shr2Q7AkIvyE",
        "colab_type": "code",
        "outputId": "ec47f0e0-bf21-4e4b-a507-7455500c48b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_test.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id          0\n",
              "Depress     0\n",
              "Chunk_1     0\n",
              "Chunk_2     0\n",
              "Chunk_3     0\n",
              "Chunk_4     0\n",
              "Chunk_5     0\n",
              "Chunk_6     0\n",
              "Chunk_7     0\n",
              "Chunk_8     0\n",
              "Chunk_9     0\n",
              "Chunk_10    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOKvVv5BKzGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Salvar datos\n",
        "df_train.to_csv('train_Depression_all_chunks_nosteem.csv',encoding='utf-8',na_rep= ' ')\n",
        "df_test.to_csv('test_Depression_all_chunks_nosteem.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW7fpvIzS6sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=pd.read_csv('train_Depression_all_chunks_nosteem.csv')\n",
        "train_df=train_df.replace(np.nan, '', regex=True)\n",
        "test_df=pd.read_csv('test_Depression_all_chunks_nosteem.csv')\n",
        "test_df=train_df.replace(np.nan, '', regex=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQJm1Wh3c8QR",
        "colab_type": "code",
        "outputId": "efdd3dc5-1542-4ee5-802e-b1838e16dc78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "train_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "Id            0\n",
              "Depress       0\n",
              "Chunk_1       0\n",
              "Chunk_2       0\n",
              "Chunk_3       0\n",
              "Chunk_4       0\n",
              "Chunk_5       0\n",
              "Chunk_6       0\n",
              "Chunk_7       0\n",
              "Chunk_8       0\n",
              "Chunk_9       0\n",
              "Chunk_10      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4UBocBjTcHg",
        "colab_type": "code",
        "outputId": "aa8dd3d1-1b62-4f61-9412-11adb796996e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unnamed: 0    0\n",
              "Id            0\n",
              "Depress       0\n",
              "Chunk_1       0\n",
              "Chunk_2       0\n",
              "Chunk_3       0\n",
              "Chunk_4       0\n",
              "Chunk_5       0\n",
              "Chunk_6       0\n",
              "Chunk_7       0\n",
              "Chunk_8       0\n",
              "Chunk_9       0\n",
              "Chunk_10      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}